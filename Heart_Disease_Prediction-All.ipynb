{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacd8920",
   "metadata": {},
   "source": [
    "# Logistic Regression, Random Forest, Decision Tree, KNN, N-Bayes, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9512a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression\n",
      "Training Accuracy: 0.8512396694214877\n",
      "Testing Accuracy: 0.819672131147541\n",
      "Confusion Matrix =\n",
      " [[23  5]\n",
      " [ 6 27]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81        28\n",
      "           1       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.82        61\n",
      "   macro avg       0.82      0.82      0.82        61\n",
      "weighted avg       0.82      0.82      0.82        61\n",
      "\n",
      "\n",
      "Training Random Forest\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7868852459016393\n",
      "Confusion Matrix =\n",
      " [[22  6]\n",
      " [ 7 26]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77        28\n",
      "           1       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.79        61\n",
      "   macro avg       0.79      0.79      0.79        61\n",
      "weighted avg       0.79      0.79      0.79        61\n",
      "\n",
      "\n",
      "Training Decision Tree\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7704918032786885\n",
      "Confusion Matrix =\n",
      " [[21  7]\n",
      " [ 7 26]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        28\n",
      "           1       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.77        61\n",
      "   macro avg       0.77      0.77      0.77        61\n",
      "weighted avg       0.77      0.77      0.77        61\n",
      "\n",
      "\n",
      "Training KNN\n",
      "Training Accuracy: 0.78099173553719\n",
      "Testing Accuracy: 0.6229508196721312\n",
      "Confusion Matrix =\n",
      " [[16 12]\n",
      " [11 22]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58        28\n",
      "           1       0.65      0.67      0.66        33\n",
      "\n",
      "    accuracy                           0.62        61\n",
      "   macro avg       0.62      0.62      0.62        61\n",
      "weighted avg       0.62      0.62      0.62        61\n",
      "\n",
      "\n",
      "Training Naive Bayes\n",
      "Training Accuracy: 0.8471074380165289\n",
      "Testing Accuracy: 0.819672131147541\n",
      "Confusion Matrix =\n",
      " [[25  3]\n",
      " [ 8 25]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82        28\n",
      "           1       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.82        61\n",
      "   macro avg       0.83      0.83      0.82        61\n",
      "weighted avg       0.83      0.82      0.82        61\n",
      "\n",
      "\n",
      "Training SVM\n",
      "Training Accuracy: 0.6942148760330579\n",
      "Testing Accuracy: 0.6229508196721312\n",
      "Confusion Matrix =\n",
      " [[13 15]\n",
      " [ 8 25]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.46      0.53        28\n",
      "           1       0.62      0.76      0.68        33\n",
      "\n",
      "    accuracy                           0.62        61\n",
      "   macro avg       0.62      0.61      0.61        61\n",
      "weighted avg       0.62      0.62      0.61        61\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KHSIM\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Data Collection and Data Analysis\n",
    "heart_ds = pd.read_csv('heart.csv')\n",
    "\n",
    "# Data Pre - Processing\n",
    "X = heart_ds.drop(columns='target', axis=1)\n",
    "Y = heart_ds['target']\n",
    "\n",
    "# Splitting the data into Training and Testing data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=2),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=2),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "# Training and Evaluation\n",
    "for name, model in models.items():\n",
    "    print(\"Training\", name)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Training Accuracy\n",
    "    train_pred = model.predict(X_train)\n",
    "    train_acc = accuracy_score(train_pred, Y_train)\n",
    "    print('Training Accuracy:', train_acc)\n",
    "    \n",
    "    # Testing Accuracy\n",
    "    test_pred = model.predict(X_test)\n",
    "    test_acc = accuracy_score(test_pred, Y_test)\n",
    "    print('Testing Accuracy:', test_acc)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(Y_test, test_pred)\n",
    "    print(\"Confusion Matrix =\\n\", conf_matrix)\n",
    "    \n",
    "    # Classification Report\n",
    "    class_report = classification_report(Y_test, test_pred)\n",
    "    print(\"Classification Report =\\n\", class_report)\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db45cbce",
   "metadata": {},
   "source": [
    "# Logistic Regression, Random Forest, Decision Tree, KNN, N-Bayes, SVM (Changed Version of the Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7673b",
   "metadata": {},
   "source": [
    "In this code, I added feature scaling using StandardScaler to standardize the features, which can improve the performance of some models. For SVM, I performed hyperparameter tuning using GridSearchCV to find the best parameters for the model. This process helps in optimizing the model's performance. You can further experiment with different hyperparameters or preprocessing techniques to achieve even better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cc0a5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression\n",
      "Training Accuracy: 0.8471074380165289\n",
      "Testing Accuracy: 0.7868852459016393\n",
      "Confusion Matrix =\n",
      " [[22  6]\n",
      " [ 7 26]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.77        28\n",
      "           1       0.81      0.79      0.80        33\n",
      "\n",
      "    accuracy                           0.79        61\n",
      "   macro avg       0.79      0.79      0.79        61\n",
      "weighted avg       0.79      0.79      0.79        61\n",
      "\n",
      "\n",
      "Training Random Forest\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7704918032786885\n",
      "Confusion Matrix =\n",
      " [[22  6]\n",
      " [ 8 25]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76        28\n",
      "           1       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.77        61\n",
      "   macro avg       0.77      0.77      0.77        61\n",
      "weighted avg       0.77      0.77      0.77        61\n",
      "\n",
      "\n",
      "Training Decision Tree\n",
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7704918032786885\n",
      "Confusion Matrix =\n",
      " [[21  7]\n",
      " [ 7 26]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        28\n",
      "           1       0.79      0.79      0.79        33\n",
      "\n",
      "    accuracy                           0.77        61\n",
      "   macro avg       0.77      0.77      0.77        61\n",
      "weighted avg       0.77      0.77      0.77        61\n",
      "\n",
      "\n",
      "Training KNN\n",
      "Training Accuracy: 0.8677685950413223\n",
      "Testing Accuracy: 0.819672131147541\n",
      "Confusion Matrix =\n",
      " [[23  5]\n",
      " [ 6 27]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81        28\n",
      "           1       0.84      0.82      0.83        33\n",
      "\n",
      "    accuracy                           0.82        61\n",
      "   macro avg       0.82      0.82      0.82        61\n",
      "weighted avg       0.82      0.82      0.82        61\n",
      "\n",
      "\n",
      "Training Naive Bayes\n",
      "Training Accuracy: 0.8471074380165289\n",
      "Testing Accuracy: 0.819672131147541\n",
      "Confusion Matrix =\n",
      " [[25  3]\n",
      " [ 8 25]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82        28\n",
      "           1       0.89      0.76      0.82        33\n",
      "\n",
      "    accuracy                           0.82        61\n",
      "   macro avg       0.83      0.83      0.82        61\n",
      "weighted avg       0.83      0.82      0.82        61\n",
      "\n",
      "\n",
      "Training SVM\n",
      "Training Accuracy: 0.8925619834710744\n",
      "Testing Accuracy: 0.8360655737704918\n",
      "Confusion Matrix =\n",
      " [[22  6]\n",
      " [ 4 29]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        28\n",
      "           1       0.83      0.88      0.85        33\n",
      "\n",
      "    accuracy                           0.84        61\n",
      "   macro avg       0.84      0.83      0.83        61\n",
      "weighted avg       0.84      0.84      0.84        61\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Data Collection and Data Analysis\n",
    "heart_ds = pd.read_csv('heart.csv')\n",
    "\n",
    "# Data Pre - Processing\n",
    "X = heart_ds.drop(columns='target', axis=1)\n",
    "Y = heart_ds['target']\n",
    "\n",
    "# Splitting the data into Training and Testing data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=2),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=2),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Training and Evaluation\n",
    "for name, model in models.items():\n",
    "    print(\"Training\", name)\n",
    "    if name == 'SVM':\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train_scaled, Y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_model.fit(X_train_scaled, Y_train)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, Y_train)\n",
    "        best_model = model\n",
    "    \n",
    "    # Training Accuracy\n",
    "    train_pred = best_model.predict(X_train_scaled)\n",
    "    train_acc = accuracy_score(train_pred, Y_train)\n",
    "    print('Training Accuracy:', train_acc)\n",
    "    \n",
    "    # Testing Accuracy\n",
    "    test_pred = best_model.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(test_pred, Y_test)\n",
    "    print('Testing Accuracy:', test_acc)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(Y_test, test_pred)\n",
    "    print(\"Confusion Matrix =\\n\", conf_matrix)\n",
    "    \n",
    "    # Classification Report\n",
    "    class_report = classification_report(Y_test, test_pred)\n",
    "    print(\"Classification Report =\\n\", class_report)\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710ba24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
